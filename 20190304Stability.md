# Read Note for [Stability and Generalization of Learning Algorithms that Converge to Global Optima](https://arxiv.org/pdf/1710.08402.pdf)
## Summary
In this paper, the autors establish generalization bounds for learning algorithms that converge to global minima.
1. The results are established for nonconvex functions satisfying the Polyak-Lojasieewicz and the quadratic growth condition.
2. They use the black-box results to establish the stability of optimization algorithms.
3. They show comparable stability for SGD and GD in PL setting.
4. There exist simple neural networs with multiple local minima where SGD is table but GD is not.


