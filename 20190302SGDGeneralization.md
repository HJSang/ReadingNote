# Read Note [Generalization Error Bounds with Probabilistic Guarantee for SGD in Nonconvex Optimization](https://arxiv.org/pdf/1802.06903.pdf)

## Summary
This paper studies the generalization of SGD with probabilistic guarantee using stability property:
1. Existing works based on stability have been established [in expectation](https://arxiv.org/pdf/1509.01240.pdf).
2. For nonconvex function and gradient dominant loss functions, they characterize the on-average stability of the iterates generated by SGD in terms of the on-average variance of the stochastic gradients.
3. They claim that such characterization leads to improved generalization error bounds.
4. Also establish generalization error bound for proximal SGD.

## Introduction
1. z iid sample data; l(., z)is the loss function
2. The emperical risk minimization (ERM) problem is to minimize f_S(w) = 1/n \sum_{i=1}^n l(w,z_i)
3. The generalization error is | f_S(w_S) - f(w_S)|, where f(w) = E_S{f_S(w)} expectation w.r.t sample S.
4. For a learning algorithm A, its stability corresponds to how stable the output of the algorithm is with regard to the variants in the data set. The generalization error can be bounded by uniform stability bound.
5. Two metrics are taken for measuring generalization error. One is the generalizaton in expectation w.r.t sample S and algorithm A. The second one is the generalization error with probabilistic guarantee, i.e., for any epsilon > 0, the quality P(|f_S(w_{T,S})- f(w_{T,S})| < \epsilon) converges to one as n goes to infinity. The generalization error in expectation implies the generalization in probability. 
