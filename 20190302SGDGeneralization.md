# Read Note [Generalization Error Bounds with Probabilistic Guarantee for SGD in Nonconvex Optimization](https://arxiv.org/pdf/1802.06903.pdf)

## Summary
This paper studies the generalization of SGD with probabilistic guarantee using stability property:
1. Existing works based on stability have been established [in expectation](https://arxiv.org/pdf/1509.01240.pdf).
2. For nonconvex function and gradient dominant loss functions, they characterize the on-average stability of the iterates generated by SGD in terms of the on-average variance of the stochastic gradients.
3. They claim that such characterization leads to improved generalization error bounds.
4. Also establish generalization error bound for proximal SGD.
