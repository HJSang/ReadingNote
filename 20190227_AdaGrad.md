Reading note for paper "[The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning)" 

In this paper, the authors show that AdaGrad, RMSProp, and Adam, under simple overparameterized problem, **generalize worse** than SGD.  For an inllustartive binary classification problem where the data is linearly seperable, SD and SGD achieves zero test error, while adaptive gradients attain errors close to half.

For adaptive gradients, the autors provide a uniform representation: $w_{k+1} = w_k - \alpha_k H_k^{-1}\nabla f(w_k+\gamma_k(w_k-w_{k-1})) + \beta_k H_k^{-1}H_{k-1} (w_k-w_{k-1})$. H_k is a diagonal matric with $Diag(\sum g_kg_k^T)$.
In this paper, authors refer generalization to classification error in test data.

The binary classification example is described as follows: minimize_w $R_S[w] = 1/2 ||Xw- y||^2_2$. X is an $n\times d$ matrix features and $y$ is a n-dimensional vector on {1, -1}. When d > n, if there is a minimizer with loss 0, then there exist an infinite number of global minimizers. 

For non-adaptive algorithms, starting from any initial,  the final solution is the minimum norm solution, which has the largest margin out of all solutions of the equation $Xw=y$.

For adaptive gradients, the following lemma is the most key finding of this paper.
**Lemma** Suppose there exists a scalar c such that $X sign(X^Ty) = cy$. Then, when initialized at w_0 =0, adaptive algorithms converge to the unique solution $w\prop sign(X^Ty)$.

