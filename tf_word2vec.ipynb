{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf-word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6nohGqQdKuYxI8OgvhcFE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HJSang/ReadingNote/blob/master/tf_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqBgF1LVJ9ez",
        "colab_type": "text"
      },
      "source": [
        "# Implement word2vec using tensorflow\n",
        "* References:\n",
        "  - [Git project](https://github.com/chao-ji/tf-word2vec)\n",
        "  - Note that: most codes are directly copied from [chao-ji/tf-word2vec](https://github.com/chao-ji/tf-word2vec)\n",
        "  - The goal of this colab is to understand the details of word2vec.\n",
        "\n",
        "* Word2vec:\n",
        "  - [Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "  - [cs224n lecture 2](http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture02-wordvecs2.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBcmSfj1LECq",
        "colab_type": "text"
      },
      "source": [
        "## Code design \n",
        "* Data process\n",
        "  - Produce the vocubulary \n",
        "  - generate the maps: word to index and index to word\n",
        "  - Process the data using tf.data.Dataset\n",
        "\n",
        "* Train model\n",
        "  - Build the sequence model \n",
        "  - Define the hyper-parameters\n",
        "  - Start the training\n",
        "\n",
        "* Retrieve the vectors\n",
        "  - Use the word2vec for inference  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnhqaPSRKdz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import heapq\n",
        "import itertools\n",
        "import collections\n",
        "import os \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKMSGwjjNHdw",
        "colab_type": "text"
      },
      "source": [
        "## Utility function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldm7MI3yNODp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_step_signature(arch, algm, batch_size, window_size=None,\n",
        "                             max_depth=None):\n",
        "  \"\"\"Get the training step signatures for `input`, `labels`, and `progress`\n",
        "  tensor.\n",
        "\n",
        "  Args:\n",
        "    arch: string scalar, architecture ('skip_gram','cbow').\n",
        "    algm: string scalar, training algorithm ('negative_sampling' or \n",
        "    'hierachical_softmax').\n",
        "\n",
        "  Returns:\n",
        "    train_step_signature: a list of three tf.TensorSpec instances, specifying\n",
        "    the tensor spec (shape and dtype) for `input`, `labels`, and `progress`. \n",
        "  \"\"\"\n",
        "  assert arch == 'skip_gram' or arch == 'cbow'\n",
        "  if arch == 'skip_gram':\n",
        "    inputs_spec = tf.TensorSpec(shape=(batch_size,),dtype=tf.int64)\n",
        "  else:\n",
        "    inputs_spec = tf.TensorSpec(shape=(batch_size,2*window_size+1),\n",
        "                                dtype=tf.int64)\n",
        "  assert algm == 'negative_sampling' or algm == 'hierarchical_softmax'\n",
        "  if algm == 'negative_sampling':\n",
        "    labels_spec = tf.TensorSpec(shape=(batch_size,),dtype=tf.int64)\n",
        "  else:\n",
        "    labels_spec = tf.TensorSpec(shape=(batch_size,2*max_depth+1),dtype=tf.int64)\n",
        "\n",
        "  return [inputs_spec,labels_spec,\n",
        "          tf.TensorSpec(shape=(batch_size,),dtype=tf.float32)]      "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z220iDjqQ8_I",
        "colab_type": "text"
      },
      "source": [
        "## Data Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-At6_B8Q_yC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Tokenizer:\n",
        "  \"\"\"Vanilla word tokenizer that splits out space-separated tokens from raw\n",
        "  text string.\n",
        "  \"\"\"\n",
        "  def __init__(self,max_vocab_size=0,min_count=10,sample=1e-3):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      max_vocab_size: int scalar, maximum vocabulary size. if >0, only the top\n",
        "      `max_vocab_size` most frequent words will be kept in vocabulary.\n",
        "      min_count: int scalar, words whose counts <`min_count` will not be\n",
        "      included in the vocabulary.\n",
        "      sample: float scalar, subsampling rate.\n",
        "    \"\"\"\n",
        "    self._max_vocab_size = max_vocab_size\n",
        "    self._min_count = min_count\n",
        "    self._sample = sample\n",
        "\n",
        "    self._vocab = None\n",
        "    self._table_words = None\n",
        "    self._unigram_counts = None\n",
        "    self._keep_probs = None\n",
        "\n",
        "  @property\n",
        "  def unigram_counts(self):\n",
        "    return self._unigram_counts\n",
        "\n",
        "  @property\n",
        "  def table_words(self):\n",
        "    return self._table_words\n",
        "\n",
        "  def _build_raw_vocab(self, filenames):\n",
        "    \"\"\"Builds raw vocabulary by iterating through the corpus once and count the\n",
        "    unique words.\n",
        "\n",
        "    Args:\n",
        "      filenames: List of strings, holding names of text files.\n",
        "\n",
        "    Returns:\n",
        "      raw_vocab: a list of 2-tuples holding the word (string) and count (int),\n",
        "      sorted in descending order of word count.  \n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for fn in filenames:\n",
        "      with tf.io.gfile.GFile(fn) as f:\n",
        "        lines.append(f)\n",
        "    lines = itertools.chain(*lines)\n",
        "    raw_vocab = collections.Counter()\n",
        "    for line in lines:\n",
        "      raw_vocab.update(lines.strip().split())\n",
        "\n",
        "    raw_vocab = raw_vocab.most_common()\n",
        "    # Truncate to have at most `max_vocab_size` vocab words\n",
        "    if self._max_vocab_size > 0:\n",
        "      raw_vocab = raw_vocab[:self._max_vocab_size]\n",
        "    return raw_vocab\n",
        "\n",
        "  def build_vocab(self, filenames):\n",
        "    \"\"\"Builds the vocabulary.\n",
        "\n",
        "    Has the side effect of setting the following attributes: for each word, we \n",
        "    have:\n",
        "    vocab[word] = index\n",
        "    table_words[index] = word\n",
        "    unigram_counts[index] = count of word\n",
        "    keep_probs[index] = keep prob of word for subsampling\n",
        "\n",
        "    Args:\n",
        "      filenames: list of strings, holding names of text files.\n",
        "    \"\"\"\n",
        "    raw_vocab =self._build_raw_vocab(filenames)\n",
        "    raw_vocab =[(w,c) for w,c in raw_vocab if c>=self._min_count]\n",
        "    self._corpus_size = sum(list(zip(*raw_vocab))[1])\n",
        "\n",
        "    self._vocab = {}\n",
        "    self._table_words = []\n",
        "    self._unigram_counts = []\n",
        "    self._keep_probs = []\n",
        "    for index, (word, count) in enumerate(raw_vocab):\n",
        "      frac = count / float(self._corpus_size)\n",
        "      keep_prob = (np.sqrt(frac / self._sample)+1)*(self._sample/frac)\n",
        "      keep_prob = np.minimum(keep_prob,1.0)\n",
        "      self._table_words.append(word)\n",
        "      self._vocab[word] = index\n",
        "      self._unigram_counts.append(count)\n",
        "      self._keep_probs.append(keep_prob)\n",
        "\n",
        "  def encode(self,string):\n",
        "     \"\"\"Split raw text string into tokens and translate to token ids.\n",
        "\n",
        "     Args:\n",
        "       string: string scalar, the raw text string to be tokenized.\n",
        "\n",
        "     Returns:\n",
        "       ids: a list of ints, the token ids of the tokenized string.  \n",
        "     \"\"\"\n",
        "     tokens = string.strip().split()\n",
        "     ids = [self._vocab[token] if token in self._vocab else -1 for token in\n",
        "            tokens]\n",
        "     return ids       "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHZbugVQZtZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Defines word tokenizer and word2vec dataset builder.\n",
        "\"\"\"\n",
        "OOV_ID = -1\n",
        "\n",
        "\n",
        "class WordTokenizer(object):\n",
        "  \"\"\"Vanilla word tokenizer that spits out space-separated tokens from raw text \n",
        "  string. Note for non-space separated languages, the corpus must be \n",
        "  pre-tokenized such that tokens are space-delimited.\n",
        "  \"\"\"\n",
        "  def __init__(self, max_vocab_size=0, min_count=10, sample=1e-3):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      max_vocab_size: int scalar, maximum vocabulary size. If > 0, only the top \n",
        "        `max_vocab_size` most frequent words will be kept in vocabulary.\n",
        "      min_count: int scalar, words whose counts < `min_count` will not be \n",
        "        included in the vocabulary.\n",
        "      sample: float scalar, subsampling rate.\n",
        "    \"\"\"\n",
        "    self._max_vocab_size = max_vocab_size\n",
        "    self._min_count = min_count\n",
        "    self._sample = sample\n",
        "\n",
        "    self._vocab = None \n",
        "    self._table_words = None\n",
        "    self._unigram_counts = None\n",
        "    self._keep_probs = None\n",
        "\n",
        "  @property\n",
        "  def unigram_counts(self):\n",
        "    return self._unigram_counts\n",
        "\n",
        "  @property\n",
        "  def table_words(self):\n",
        "    return self._table_words\n",
        "\n",
        "  def _build_raw_vocab(self, filenames):\n",
        "    \"\"\"Builds raw vocabulary by iterate through the corpus once and count the \n",
        "    unique words.\n",
        "\n",
        "    Args:\n",
        "      filenames: list of strings, holding names of text files.\n",
        "\n",
        "    Returns: \n",
        "      raw_vocab: a list of 2-tuples holding the word (string) and count (int),\n",
        "        sorted in descending order of word count. \n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for fn in filenames:\n",
        "      with tf.io.gfile.GFile(fn) as f:\n",
        "        lines.append(f)\n",
        "    lines = itertools.chain(*lines)\n",
        "\n",
        "    raw_vocab = collections.Counter()\n",
        "    for line in lines:\n",
        "      raw_vocab.update(line.strip().split())\n",
        "    raw_vocab = raw_vocab.most_common()\n",
        "    # truncate to have at most `max_vocab_size` vocab words\n",
        "    if self._max_vocab_size > 0:\n",
        "      raw_vocab = raw_vocab[:self._max_vocab_size]\n",
        "    return raw_vocab\n",
        "   \n",
        "  def build_vocab(self, filenames):\n",
        "    \"\"\"Builds the vocabulary.\n",
        "\n",
        "    Has the side effect of setting the following attributes: for each word \n",
        "    `word` we have\n",
        "\n",
        "    vocab[word] = index\n",
        "    table_words[index] = word `word`\n",
        "    unigram_counts[index] = count of `word` in vocab\n",
        "    keep_probs[index] = keep prob of `word` for subsampling\n",
        "\n",
        "    Args:\n",
        "      filenames: list of strings, holding names of text files.\n",
        "    \"\"\"\n",
        "    raw_vocab = self._build_raw_vocab(filenames)\n",
        "    raw_vocab = [(w, c) for w, c in raw_vocab if c >= self._min_count]\n",
        "    self._corpus_size = sum(list(zip(*raw_vocab))[1])\n",
        "\n",
        "    self._vocab = {}\n",
        "    self._table_words = []\n",
        "    self._unigram_counts = []\n",
        "    self._keep_probs = []\n",
        "    for index, (word, count) in enumerate(raw_vocab):\n",
        "      frac = count / float(self._corpus_size)\n",
        "      keep_prob = (np.sqrt(frac / self._sample) + 1) * (self._sample / frac)\n",
        "      keep_prob = np.minimum(keep_prob, 1.0)\n",
        "      self._vocab[word] = index\n",
        "      self._table_words.append(word)\n",
        "      self._unigram_counts.append(count)\n",
        "      self._keep_probs.append(keep_prob)\n",
        "\n",
        "  def encode(self, string):\n",
        "    \"\"\"Split raw text string into tokens (space-separated) and tranlate to token \n",
        "    ids.\n",
        "\n",
        "    Args:\n",
        "      string: string scalar, the raw text string to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "      ids: a list of ints, the token ids of the tokenized string.\n",
        "    \"\"\"\n",
        "    tokens = string.strip().split()\n",
        "    ids = [self._vocab[token] if token in self._vocab else OOV_ID \n",
        "        for token in tokens]\n",
        "    return ids\n",
        "\n",
        "\n",
        "class Word2VecDatasetBuilder(object):\n",
        "  \"\"\"Builds a tf.data.Dataset instance that generates matrices holding word\n",
        "  indices for training Word2Vec models.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               tokenizer,\n",
        "               arch='skip_gram',\n",
        "               algm='negative_sampling',\n",
        "               epochs=1,\n",
        "               batch_size=32,\n",
        "               window_size=5):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      epochs: int scalar, num times the dataset is iterated.\n",
        "      batch_size: int scalar, the returned tensors in `get_tensor_dict` have\n",
        "        shapes [batch_size, :]. \n",
        "      window_size: int scalar, num of words on the left or right side of\n",
        "        target word within a window.\n",
        "    \"\"\"\n",
        "    self._tokenizer = tokenizer\n",
        "    self._arch = arch\n",
        "    self._algm = algm\n",
        "    self._epochs = epochs\n",
        "    self._batch_size = batch_size\n",
        "    self._window_size = window_size\n",
        "\n",
        "    self._max_depth = None\n",
        "\n",
        "  def _build_binary_tree(self, unigram_counts):\n",
        "    \"\"\"Builds a Huffman tree for hierarchical softmax. Has the side effect\n",
        "    of setting `max_depth`.\n",
        "\n",
        "    Args:\n",
        "      unigram_counts: list of int, holding word counts. Index of each entry\n",
        "        is the same as the word index into the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "      codes_points: an int numpy array of shape [vocab_size, 2*max_depth+1]\n",
        "        where each row holds the codes (0-1 binary values) padded to\n",
        "        `max_depth`, and points (non-leaf node indices) padded to `max_depth`,\n",
        "        of each vocabulary word. The last entry is the true length of code\n",
        "        and point (<= `max_depth`).\n",
        "    \"\"\"\n",
        "    vocab_size = len(unigram_counts)\n",
        "    heap = [[unigram_counts[i], i] for i in range(vocab_size)]\n",
        "    # initialize the min-priority queue, which has length `vocab_size`\n",
        "    heapq.heapify(heap)\n",
        "\n",
        "    # insert `vocab_size` - 1 internal nodes, with vocab words as leaf nodes.\n",
        "    for i in range(vocab_size - 1):\n",
        "      min1, min2 = heapq.heappop(heap), heapq.heappop(heap)\n",
        "      heapq.heappush(heap, [min1[0] + min2[0], i + vocab_size, min1, min2])\n",
        "    # At this point we have a len-1 heap, and `heap[0]` will be the root of \n",
        "    # the binary tree; where internal nodes store\n",
        "    # 1. key (frequency)\n",
        "    # 2. vocab index\n",
        "    # 3. left child\n",
        "    # 4. right child\n",
        "    # and leaf nodes store\n",
        "    # 1. key (frequencey)\n",
        "    # 2. vocab index\n",
        "\n",
        "    # Traverse the Huffman tree rooted at `heap[0]` in the order of \n",
        "    # Depth-First-Search. Each stack item stores the\n",
        "    # 1. `node`\n",
        "    # 2. code of the `node` (list)\n",
        "    # 3. point of the `node` (list)\n",
        "    #\n",
        "    # `point` is the list of vocab IDs of the internal nodes along the path from \n",
        "    # the root up to `node` (not included)\n",
        "    # `code` is the list of labels (0 or 1) of the edges along the path from the\n",
        "    # root up to `node` \n",
        "    # they are empty lists for the root node `heap[0]`\n",
        "    node_list = []\n",
        "    max_depth, stack = 0, [[heap[0], [], []]] # stack: [root, codde, point]\n",
        "    while stack:\n",
        "      node, code, point = stack.pop()\n",
        "      if node[1] < vocab_size:\n",
        "        # leaf node: len(node) == 2\n",
        "        node.extend([code, point, len(point)])\n",
        "        max_depth = np.maximum(len(code), max_depth)\n",
        "        node_list.append(node)\n",
        "      else:\n",
        "        # internal node: len(node) == 4\n",
        "        point = np.array(list(point) + [node[1]-vocab_size])\n",
        "        stack.append([node[2], np.array(list(code)+[0]), point])\n",
        "        stack.append([node[3], np.array(list(code)+[1]), point])\n",
        "\n",
        "    # `len(node_list[i]) = 5`\n",
        "    node_list = sorted(node_list, key=lambda items: items[1])\n",
        "    # Stores the padded codes and points for each vocab word\n",
        "    codes_points = np.zeros([vocab_size, max_depth*2+1], dtype=np.int64)\n",
        "    for i in range(len(node_list)):\n",
        "      length = node_list[i][4] # length of code or point\n",
        "      codes_points[i, -1] = length\n",
        "      codes_points[i, :length] = node_list[i][2] # code\n",
        "      codes_points[i, max_depth:max_depth+length] = node_list[i][3] # point\n",
        "    self._max_depth = max_depth\n",
        "    return codes_points\n",
        "\n",
        "  def build_dataset(self, filenames):\n",
        "    \"\"\"Generates tensor dict mapping from tensor names to tensors.\n",
        "\n",
        "    Args:\n",
        "      filenames: list of strings, holding names of text files.\n",
        "      \n",
        "    Returns:\n",
        "      dataset: a tf.data.Dataset instance, holding the a tuple of tensors\n",
        "        (inputs, labels, progress)\n",
        "        when arch=='skip_gram', algm=='negative_sampling'\n",
        "          inputs: [N],                    labels: [N]\n",
        "        when arch=='cbow', algm=='negative_sampling'\n",
        "          inputs: [N, 2*window_size+1],   labels: [N]\n",
        "        when arch=='skip_gram', algm=='hierarchical_softmax'\n",
        "          inputs: [N],                    labels: [N, 2*max_depth+1]\n",
        "        when arch=='cbow', algm=='hierarchical_softmax'\n",
        "          inputs: [N, 2*window_size+1],   labels: [N, 2*max_depth+1]\n",
        "        progress: [N], the percentage of sentences covered so far. Used to \n",
        "          compute learning rate.\n",
        "    \"\"\"\n",
        "    unigram_counts = self._tokenizer._unigram_counts\n",
        "    keep_probs = self._tokenizer._keep_probs\n",
        "\n",
        "    if self._algm == 'hierarchical_softmax':\n",
        "      codes_points = tf.constant(self._build_binary_tree(unigram_counts))\n",
        "    elif self._algm == 'negative_sampling':\n",
        "      codes_points = None\n",
        "    else:\n",
        "      raise ValueError('algm must be hierarchical_softmax or negative_sampling')\n",
        "   \n",
        "    keep_probs = tf.cast(tf.constant(keep_probs), 'float32')\n",
        "\n",
        "    # total num of sentences (lines) across text files times num of epochs\n",
        "    num_sents = sum([len(list(tf.io.gfile.GFile(fn))) \n",
        "        for fn in filenames]) * self._epochs\n",
        "\n",
        "    def generator_fn():\n",
        "      for _ in range(self._epochs):\n",
        "        for fn in filenames:\n",
        "          with tf.io.gfile.GFile(fn) as f:\n",
        "            for line in f:\n",
        "              yield self._tokenizer.encode(line)\n",
        "\n",
        "    # dataset: [([int], float)]\n",
        "    dataset = tf.data.Dataset.zip((\n",
        "        tf.data.Dataset.from_generator(generator_fn, tf.int64, [None]),\n",
        "        tf.data.Dataset.from_tensor_slices(tf.range(num_sents) / num_sents)))\n",
        "    # dataset: [([int], float)]\n",
        "    dataset = dataset.map(lambda indices, progress: \n",
        "        (subsample(indices, keep_probs), progress))\n",
        "    # dataset: [([int], float)]\n",
        "    dataset = dataset.filter(lambda indices, progress: \n",
        "        tf.greater(tf.size(indices), 1))  # sentence must have at least 2 tokens\n",
        "    # dataset: [((None, None), float)]\n",
        "    dataset = dataset.map(lambda indices, progress: (generate_instances(\n",
        "        indices, self._arch, self._window_size, self._max_depth, codes_points), \n",
        "        progress))\n",
        "    # dataset: [((None, None)), (None,)]\n",
        "    dataset = dataset.map(lambda instances, progress: (\n",
        "        # replicate `progress` to size `tf.shape(instances)[:1]`\n",
        "        instances, tf.fill(tf.shape(instances)[:1], progress)))\n",
        "    dataset = dataset.flat_map(lambda instances, progress: \n",
        "        # form a dataset by unstacking `instances` in the first dimension,\n",
        "        tf.data.Dataset.from_tensor_slices((instances, progress)))\n",
        "    # batch the dataset\n",
        "    dataset = dataset.batch(self._batch_size, drop_remainder=True)\n",
        "\n",
        "    def prepare_inputs_labels(tensor, progress):\n",
        "      if self._arch == 'skip_gram':\n",
        "        if self._algm == 'negative_sampling':\n",
        "          tensor.set_shape([self._batch_size, 2])\n",
        "        else:\n",
        "          tensor.set_shape([self._batch_size, 2*self._max_depth+2])\n",
        "        inputs = tensor[:, :1]\n",
        "        labels = tensor[:, 1:]\n",
        "\n",
        "      else:\n",
        "        if self._algm == 'negative_sampling':\n",
        "          tensor.set_shape([self._batch_size, 2*self._window_size+2])\n",
        "        else:\n",
        "          tensor.set_shape([self._batch_size,\n",
        "              2*self._window_size+2*self._max_depth+2])\n",
        "        inputs = tensor[:, :2*self._window_size+1]\n",
        "        labels = tensor[:, 2*self._window_size+1:]\n",
        "\n",
        "      if self._arch == 'skip_gram':\n",
        "        inputs = tf.squeeze(inputs, axis=1)\n",
        "      if self._algm == 'negative_sampling':\n",
        "        labels = tf.squeeze(labels, axis=1)\n",
        "      progress = tf.cast(progress, 'float32')\n",
        "      return inputs, labels, progress\n",
        "\n",
        "    dataset = dataset.map(lambda tensor, progress: \n",
        "        prepare_inputs_labels(tensor, progress))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def subsample(indices, keep_probs):\n",
        "  \"\"\"Filters out-of-vocabulary words and then applies subsampling on words in a \n",
        "  sentence. Words with high frequencies have lower keep probs.\n",
        "\n",
        "  Args:\n",
        "    indices: rank-1 int tensor, the word indices within a sentence.\n",
        "    keep_probs: rank-1 float tensor, the prob to drop the each vocabulary word. \n",
        "\n",
        "  Returns:\n",
        "    indices: rank-1 int tensor, the word indices within a sentence after \n",
        "      subsampling.\n",
        "  \"\"\"\n",
        "  indices = tf.boolean_mask(indices, tf.not_equal(indices, OOV_ID))\n",
        "  keep_probs = tf.gather(keep_probs, indices)\n",
        "  randvars = tf.random.uniform(tf.shape(keep_probs), 0, 1)\n",
        "  indices = tf.boolean_mask(indices, tf.less(randvars, keep_probs))\n",
        "  return indices\n",
        "\n",
        "\n",
        "def generate_instances(\n",
        "    indices, arch, window_size, max_depth=None, codes_points=None):\n",
        "  \"\"\"Generates matrices holding word indices to be passed to Word2Vec models \n",
        "  for each sentence. The shape and contents of output matrices depends on the \n",
        "  architecture ('skip_gram', 'cbow') and training algorithm ('negative_sampling'\n",
        "  , 'hierarchical_softmax').\n",
        "\n",
        "  It takes as input a list of word indices in a subsampled-sentence, where each\n",
        "  word is a target word, and their context words are those within the window \n",
        "  centered at a target word. For skip gram architecture, `num_context_words` \n",
        "  instances are generated for a target word, and for cbow architecture, a single\n",
        "  instance is generated for a target word.\n",
        "\n",
        "  If `codes_points` is not None ('hierarchical softmax'), the word to be \n",
        "  predicted (context word for 'skip_gram', and target word for 'cbow') are \n",
        "  represented by their 'codes' and 'points' in the Huffman tree (See \n",
        "  `_build_binary_tree`). \n",
        "\n",
        "  Args:\n",
        "    indices: rank-1 int tensor, the word indices within a sentence after\n",
        "      subsampling.\n",
        "    arch: scalar string, architecture ('skip_gram' or 'cbow').\n",
        "    window_size: int scalar, num of words on the left or right side of\n",
        "      target word within a window.\n",
        "    max_depth: (Optional) int scalar, the max depth of the Huffman tree. \n",
        "    codes_points: (Optional) an int tensor of shape [vocab_size, 2*max_depth+1] \n",
        "      where each row holds the codes (0-1 binary values) padded to `max_depth`, \n",
        "      and points (non-leaf node indices) padded to `max_depth`, of each \n",
        "      vocabulary word. The last entry is the true length of code and point \n",
        "      (<= `max_depth`).\n",
        "    \n",
        "  Returns:\n",
        "    instances: an int tensor holding word indices, with shape being\n",
        "      when arch=='skip_gram', algm=='negative_sampling'\n",
        "        shape: [N, 2]\n",
        "      when arch=='cbow', algm=='negative_sampling'\n",
        "        shape: [N, 2*window_size+2]\n",
        "      when arch=='skip_gram', algm=='hierarchical_softmax'\n",
        "        shape: [N, 2*max_depth+2]\n",
        "      when arch=='cbow', algm='hierarchical_softmax'\n",
        "        shape: [N, 2*window_size+2*max_depth+2]\n",
        "  \"\"\"\n",
        "  def per_target_fn(index, init_array):\n",
        "    \"\"\"Generate inputs and labels for each target word.\n",
        "\n",
        "    `index` is the index of the target word in `indices`.\n",
        "    \"\"\"\n",
        "    reduced_size = tf.random.uniform([], maxval=window_size, dtype='int32')\n",
        "    left = tf.range(tf.maximum(index - window_size + reduced_size, 0), index)\n",
        "    right = tf.range(index + 1, \n",
        "        tf.minimum(index + 1 + window_size - reduced_size, tf.size(indices)))\n",
        "    context = tf.concat([left, right], axis=0)\n",
        "    context = tf.gather(indices, context)\n",
        "\n",
        "    if arch == 'skip_gram':\n",
        "      # replicate `indices[index]` to match the size of `context`\n",
        "      # [N, 2]\n",
        "      window = tf.stack([tf.fill(tf.shape(context), indices[index]), \n",
        "                        context], axis=1)\n",
        "    elif arch == 'cbow':\n",
        "      true_size = tf.size(context)\n",
        "      # pad `context` to length `2 * window_size`\n",
        "      window = tf.concat([tf.pad(context, [[0, 2*window_size-true_size]]), \n",
        "                          [true_size, indices[index]]], axis=0)\n",
        "      # [1, 2*window_size + 2]\n",
        "      window = tf.expand_dims(window, axis=0)\n",
        "    else:\n",
        "      raise ValueError('architecture must be skip_gram or cbow.')\n",
        "\n",
        "    if codes_points is not None:\n",
        "      # [N, 2*max_depth + 2] or [1, 2*window_size+2*max_depth+2]\n",
        "      window = tf.concat([window[:, :-1], \n",
        "                          tf.gather(codes_points, window[:, -1])], axis=1)\n",
        "    return index + 1, init_array.write(index, window)\n",
        "\n",
        "  size = tf.size(indices)\n",
        "  # initialize a tensor array of length `tf.size(indices)`\n",
        "  init_array = tf.TensorArray('int64', size=size, infer_shape=False)\n",
        "  _, result_array = tf.while_loop(lambda i, ta: i < size,\n",
        "                                  per_target_fn, \n",
        "                                  [0, init_array],\n",
        "                                      back_prop=False)\n",
        "  instances = tf.cast(result_array.concat(), 'int64')\n",
        "  if arch == 'skip_gram':\n",
        "    if max_depth is None:\n",
        "      instances.set_shape([None, 2])\n",
        "    else:\n",
        "      instances.set_shape([None, 2*max_depth+2])\n",
        "  else:\n",
        "    if max_depth is None:\n",
        "      instances.set_shape([None, 2*window_size+2])\n",
        "    else:\n",
        "      instances.set_shape([None, 2*window_size+2*max_depth+2])\n",
        "\n",
        "  return instances\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cKFJ8p_RPBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Defines word2vec model using tf.keras API.\n",
        "\"\"\"\n",
        "class Word2VecModel(tf.keras.Model):\n",
        "  \"\"\"Word2Vec model.\"\"\"\n",
        "  def __init__(self, \n",
        "               unigram_counts, \n",
        "               arch='skip_gram',\n",
        "               algm='negative_sampling', \n",
        "               hidden_size=300, \n",
        "               batch_size=256, \n",
        "               negatives=5, \n",
        "               power=0.75,\n",
        "               alpha=0.025,\n",
        "               min_alpha=0.0001,\n",
        "               add_bias=True,\n",
        "               random_seed=0):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      unigram_counts: a list of ints, the counts of word tokens in the corpus. \n",
        "      arch: string scalar, architecture ('skip_gram' or 'cbow').\n",
        "      algm: string scalar, training algorithm ('negative_sampling' or\n",
        "        'hierarchical_softmax').\n",
        "      hidden_size: int scalar, length of word vector.\n",
        "      batch_size: int scalar, batch size.\n",
        "      negatives: int scalar, num of negative words to sample.\n",
        "      power: float scalar, distortion for negative sampling. \n",
        "      alpha: float scalar, initial learning rate.\n",
        "      min_alpha: float scalar, final learning rate.\n",
        "      add_bias: bool scalar, whether to add bias term to dotproduct \n",
        "        between syn0 and syn1 vectors.\n",
        "      random_seed: int scalar, random_seed.\n",
        "    \"\"\"\n",
        "    super(Word2VecModel, self).__init__()\n",
        "    self._unigram_counts = unigram_counts\n",
        "    self._arch = arch\n",
        "    self._algm = algm\n",
        "    self._hidden_size = hidden_size\n",
        "    self._vocab_size = len(unigram_counts)\n",
        "    self._batch_size = batch_size\n",
        "    self._negatives = negatives\n",
        "    self._power = power\n",
        "    self._alpha = alpha\n",
        "    self._min_alpha = min_alpha\n",
        "    self._add_bias = add_bias\n",
        "    self._random_seed = random_seed\n",
        "\n",
        "    self._input_size = (self._vocab_size if self._algm == 'negative_sampling'\n",
        "                            else self._vocab_size - 1)\n",
        "\n",
        "    self.add_weight('syn0',\n",
        "                    shape=[self._vocab_size, self._hidden_size],\n",
        "                    initializer=tf.keras.initializers.RandomUniform(\n",
        "                        minval=-0.5/self._hidden_size,\n",
        "                        maxval=0.5/self._hidden_size))\n",
        "    \n",
        "    self.add_weight('syn1',\n",
        "                    shape=[self._input_size, self._hidden_size],\n",
        "                    initializer=tf.keras.initializers.RandomUniform(\n",
        "                        minval=-0.1, maxval=0.1))\n",
        "\n",
        "    self.add_weight('biases', \n",
        "                    shape=[self._input_size], \n",
        "                    initializer=tf.keras.initializers.Zeros()) \n",
        "\n",
        "  def call(self, inputs, labels):\n",
        "    \"\"\"Runs the forward pass to compute loss.\n",
        "\n",
        "    Args:\n",
        "      inputs: int tensor of shape [batch_size] (skip_gram) or \n",
        "        [batch_size, 2*window_size+1] (cbow) \n",
        "      labels: int tensor of shape [batch_size] (negative_sampling) or\n",
        "        [batch_size, 2*max_depth+1] (hierarchical_softmax)\n",
        "\n",
        "    Returns:\n",
        "      loss: float tensor, cross entropy loss. \n",
        "    \"\"\"\n",
        "    if self._algm == 'negative_sampling':\n",
        "      loss = self._negative_sampling_loss(inputs, labels)\n",
        "    elif self._algm == 'hierarchical_softmax':\n",
        "      loss = self._hierarchical_softmax_loss(inputs, labels)\n",
        "    return loss\n",
        " \n",
        "  def _negative_sampling_loss(self, inputs, labels):\n",
        "    \"\"\"Builds the loss for negative sampling.\n",
        "\n",
        "    Args:\n",
        "      inputs: int tensor of shape [batch_size] (skip_gram) or \n",
        "        [batch_size, 2*window_size+1] (cbow)\n",
        "      labels: int tensor of shape [batch_size]\n",
        "\n",
        "    Returns:\n",
        "      loss: float tensor of shape [batch_size, negatives + 1].\n",
        "    \"\"\"\n",
        "    _, syn1, biases = self.weights\n",
        "\n",
        "    sampled_values = tf.random.fixed_unigram_candidate_sampler(\n",
        "        true_classes=tf.expand_dims(labels, 1),\n",
        "        num_true=1,\n",
        "        num_sampled=self._batch_size*self._negatives,\n",
        "        unique=True,\n",
        "        range_max=len(self._unigram_counts),\n",
        "        distortion=self._power,\n",
        "        unigrams=self._unigram_counts)\n",
        "\n",
        "    sampled = sampled_values.sampled_candidates\n",
        "    sampled_mat = tf.reshape(sampled, [self._batch_size, self._negatives])\n",
        "    inputs_syn0 = self._get_inputs_syn0(inputs) # [batch_size, hidden_size]\n",
        "    true_syn1 = tf.gather(syn1, labels) # [batch_size, hidden_size]\n",
        "    # [batch_size, negatives, hidden_size]\n",
        "    sampled_syn1 = tf.gather(syn1, sampled_mat)\n",
        "    # [batch_size]\n",
        "    true_logits = tf.reduce_sum(tf.multiply(inputs_syn0, true_syn1), 1)\n",
        "    # [batch_size, negatives]\n",
        "    sampled_logits = tf.einsum('ijk,ikl->il', tf.expand_dims(inputs_syn0, 1), \n",
        "        tf.transpose(sampled_syn1, (0, 2, 1)))\n",
        "\n",
        "    if self._add_bias:\n",
        "      # [batch_size]\n",
        "      true_logits += tf.gather(biases, labels)\n",
        "      # [batch_size, negatives]\n",
        "      sampled_logits += tf.gather(biases, sampled_mat)\n",
        "\n",
        "    # [batch_size]\n",
        "    true_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "        labels=tf.ones_like(true_logits), logits=true_logits)\n",
        "    # [batch_size, negatives]\n",
        "    sampled_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "        labels=tf.zeros_like(sampled_logits), logits=sampled_logits)\n",
        "\n",
        "    loss = tf.concat(\n",
        "        [tf.expand_dims(true_cross_entropy, 1), sampled_cross_entropy], 1)\n",
        "    return loss\n",
        "\n",
        "  def _hierarchical_softmax_loss(self, inputs, labels):\n",
        "    \"\"\"Builds the loss for hierarchical softmax.\n",
        "\n",
        "    Args:\n",
        "      inputs: int tensor of shape [batch_size] (skip_gram) or \n",
        "        [batch_size, 2*window_size+1] (cbow)\n",
        "      labels: int tensor of shape [batch_size, 2*max_depth+1]\n",
        "\n",
        "    Returns:\n",
        "      loss: float tensor of shape [sum_of_code_len]\n",
        "    \"\"\"\n",
        "    _, syn1, biases = self.weights\n",
        "\n",
        "    inputs_syn0_list = tf.unstack(self._get_inputs_syn0(inputs))\n",
        "    codes_points_list = tf.unstack(labels)\n",
        "    max_depth = (labels.shape.as_list()[1] - 1) // 2\n",
        "    loss = []\n",
        "    for i in range(self._batch_size):\n",
        "      inputs_syn0 = inputs_syn0_list[i] # [hidden_size]\n",
        "      codes_points = codes_points_list[i] # [2*max_depth+1]\n",
        "      true_size = codes_points[-1]\n",
        "\n",
        "      codes = codes_points[:true_size]\n",
        "      points = codes_points[max_depth:max_depth+true_size]\n",
        "      logits = tf.reduce_sum(\n",
        "          tf.multiply(inputs_syn0, tf.gather(syn1, points)), 1)\n",
        "      if self._add_bias:\n",
        "        logits += tf.gather(biases, points)\n",
        "\n",
        "      # [true_size]\n",
        "      loss.append(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "          labels=tf.cast(codes, 'float32'), logits=logits))\n",
        "    loss = tf.concat(loss, axis=0)\n",
        "    return loss\n",
        "\n",
        "  def _get_inputs_syn0(self, inputs):\n",
        "    \"\"\"Builds the activations of hidden layer given input words embeddings \n",
        "    `syn0` and input word indices.\n",
        "\n",
        "    Args:\n",
        "      inputs: int tensor of shape [batch_size] (skip_gram) or \n",
        "        [batch_size, 2*window_size+1] (cbow)\n",
        "\n",
        "    Returns:\n",
        "      inputs_syn0: [batch_size, hidden_size]\n",
        "    \"\"\"\n",
        "    # syn0: [vocab_size, hidden_size]\n",
        "    syn0, _, _ = self.weights\n",
        "    if self._arch == 'skip_gram':\n",
        "      inputs_syn0 = tf.gather(syn0, inputs) # [batch_size, hidden_size]\n",
        "    else:\n",
        "      inputs_syn0 = []\n",
        "      contexts_list = tf.unstack(inputs)\n",
        "      for i in range(self._batch_size):\n",
        "        contexts = contexts_list[i]\n",
        "        context_words = contexts[:-1]\n",
        "        true_size = contexts[-1]\n",
        "        inputs_syn0.append(\n",
        "            tf.reduce_mean(tf.gather(syn0, context_words[:true_size]), axis=0))\n",
        "      inputs_syn0 = tf.stack(inputs_syn0)\n",
        "\n",
        "    return inputs_syn0\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGd1rj4sUMxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flags.DEFINE_string('arch', 'skip_gram', 'Architecture (skip_gram or cbow).')\n",
        "flags.DEFINE_string('algm', 'negative_sampling', 'Training algorithm '\n",
        "    '(negative_sampling or hierarchical_softmax).')\n",
        "flags.DEFINE_integer('epochs', 1, 'Num of epochs to iterate thru corpus.')\n",
        "flags.DEFINE_integer('batch_size', 256, 'Batch size.')\n",
        "flags.DEFINE_integer('max_vocab_size', 0, 'Maximum vocabulary size. If > 0, '\n",
        "    'the top `max_vocab_size` most frequent words will be kept in vocabulary.')\n",
        "flags.DEFINE_integer('min_count', 10, 'Words whose counts < `min_count` will '\n",
        "    'not be included in the vocabulary.')\n",
        "flags.DEFINE_float('sample', 1e-3, 'Subsampling rate.')\n",
        "flags.DEFINE_integer('window_size', 10, 'Num of words on the left or right side'\n",
        "    ' of target word within a window.')\n",
        "\n",
        "flags.DEFINE_integer('hidden_size', 300, 'Length of word vector.')\n",
        "flags.DEFINE_integer('negatives', 5, 'Num of negative words to sample.')\n",
        "flags.DEFINE_float('power', 0.75, 'Distortion for negative sampling.')\n",
        "flags.DEFINE_float('alpha', 0.025, 'Initial learning rate.')\n",
        "flags.DEFINE_float('min_alpha', 0.0001, 'Final learning rate.')\n",
        "flags.DEFINE_boolean('add_bias', True, 'Whether to add bias term to dotproduct '\n",
        "    'between syn0 and syn1 vectors.')\n",
        "\n",
        "flags.DEFINE_integer('log_per_steps', 10000, 'Every `log_per_steps` steps to '\n",
        "    ' log the value of loss to be minimized.')\n",
        "flags.DEFINE_list(\n",
        "    'filenames', None, 'Names of comma-separated input text files.')\n",
        "flags.DEFINE_string('out_dir', '/tmp/word2vec', 'Output directory.')\n",
        "\n",
        "FLAGS = flags.FLAGS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFNVgU1rUFHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def main(_):  \n",
        "  arch = FLAGS.arch\n",
        "  algm = FLAGS.algm\n",
        "  epochs = FLAGS.epochs\n",
        "  batch_size = FLAGS.batch_size\n",
        "  max_vocab_size = FLAGS.max_vocab_size\n",
        "  min_count = FLAGS.min_count\n",
        "  sample = FLAGS.sample\n",
        "  window_size = FLAGS.window_size\n",
        "  hidden_size = FLAGS.hidden_size\n",
        "  negatives = FLAGS.negatives\n",
        "  power = FLAGS.power\n",
        "  alpha = FLAGS.alpha\n",
        "  min_alpha = FLAGS.min_alpha\n",
        "  add_bias = FLAGS.add_bias\n",
        "  log_per_steps = FLAGS.log_per_steps\n",
        "  filenames = FLAGS.filenames\n",
        "  out_dir = FLAGS.out_dir\n",
        "\n",
        "  tokenizer = WordTokenizer(\n",
        "      max_vocab_size=max_vocab_size, min_count=min_count, sample=sample)\n",
        "  tokenizer.build_vocab(filenames)\n",
        "\n",
        "  builder = Word2VecDatasetBuilder(tokenizer,\n",
        "                                   arch=arch,\n",
        "                                   algm=algm,\n",
        "                                   epochs=epochs,\n",
        "                                   batch_size=batch_size,\n",
        "                                   window_size=window_size)\n",
        "  dataset = builder.build_dataset(filenames)\n",
        "  word2vec = Word2VecModel(tokenizer.unigram_counts,\n",
        "               arch=arch,\n",
        "               algm=algm,\n",
        "               hidden_size=hidden_size,\n",
        "               batch_size=batch_size,\n",
        "               negatives=negatives,\n",
        "               power=power,\n",
        "               alpha=alpha,\n",
        "               min_alpha=min_alpha,\n",
        "               add_bias=add_bias)\n",
        "\n",
        "  train_step_signature = utils.get_train_step_signature(\n",
        "      arch, algm, batch_size, window_size, builder._max_depth)\n",
        "  optimizer = tf.keras.optimizers.SGD(1.0)\n",
        "\n",
        "  @tf.function(input_signature=train_step_signature)\n",
        "  def train_step(inputs, labels, progress):\n",
        "    loss = word2vec(inputs, labels)\n",
        "    gradients = tf.gradients(loss, word2vec.trainable_variables)\n",
        "  \n",
        "    learning_rate = tf.maximum(alpha * (1 - progress[0]) +\n",
        "        min_alpha * progress[0], min_alpha)\n",
        "\n",
        "    if hasattr(gradients[0], '_values'):\n",
        "      gradients[0]._values *= learning_rate\n",
        "    else:\n",
        "      gradients[0] *= learning_rate\n",
        "\n",
        "    if hasattr(gradients[1], '_values'):\n",
        "      gradients[1]._values *= learning_rate\n",
        "    else:\n",
        "      gradients[1] *= learning_rate\n",
        "\n",
        "    if hasattr(gradients[2], '_values'):\n",
        "      gradients[2]._values *= learning_rate\n",
        "    else:\n",
        "      gradients[2] *= learning_rate\n",
        "\n",
        "    optimizer.apply_gradients(\n",
        "        zip(gradients, word2vec.trainable_variables))\n",
        "\n",
        "    return loss, learning_rate\n",
        "\n",
        "  average_loss = 0.\n",
        "  for step, (inputs, labels, progress) in enumerate(dataset):\n",
        "    loss, learning_rate = train_step(inputs, labels, progress)\n",
        "    average_loss += loss.numpy().mean()\n",
        "    if step % log_per_steps == 0:\n",
        "      if step > 0:\n",
        "        average_loss /= log_per_steps\n",
        "      print('step:', step, 'average_loss:', average_loss,\n",
        "            'learning_rate:', learning_rate.numpy())\n",
        "      average_loss = 0.\n",
        "\n",
        "  syn0_final = word2vec.weights[0].numpy()\n",
        "  np.save(os.path.join(FLAGS.out_dir, 'syn0_final'), syn0_final)\n",
        "  with tf.io.gfile.GFile(os.path.join(FLAGS.out_dir, 'vocab.txt'), 'w') as f:\n",
        "    for w in tokenizer.table_words:\n",
        "      f.write(w + '\\n')\n",
        "  print('Word embeddings saved to', \n",
        "      os.path.join(FLAGS.out_dir, 'syn0_final.npy'))\n",
        "  print('Vocabulary saved to', os.path.join(FLAGS.out_dir, 'vocab.txt'))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  flags.mark_flag_as_required('filenames')\n",
        "  app.run(main)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}